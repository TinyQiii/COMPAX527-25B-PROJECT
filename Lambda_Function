import os
import io
import csv
import json
import gzip
import uuid
import time
import hashlib
import logging
import datetime as dt
from urllib.parse import urlparse
from urllib.request import Request, urlopen

import boto3
from botocore.exceptions import ClientError

# ---------- Config via environment ----------
DDB_TABLE = os.environ["DDB_TABLE"]                 # e.g. InfectionReports
RAW_BUCKET = os.environ["RAW_BUCKET"]               # e.g. infection-datasets-compx527
DEFAULT_SOURCE_URL = os.environ.get("SOURCE_URL", "")  # optional default
UPSERT_MODE = os.environ.get("UPSERT_MODE", "overwrite").lower()  # "overwrite" or "skip"

# ---------- AWS clients ----------
s3 = boto3.client("s3")
ddb = boto3.resource("dynamodb").Table(DDB_TABLE)

# ---------- Logging ----------
log = logging.getLogger()
if not log.handlers:
    logging.basicConfig(level=logging.INFO)
log.setLevel(logging.INFO)

def _now_utc():
    return dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)

def _guess_content_type_from_key(key: str) -> str:
    lk = key.lower()
    if lk.endswith(".json") or ".json" in lk:
        return "json"
    return "csv"

def _read_http_bytes(url: str) -> bytes:
    log.info(f"Fetching HTTP(S) source: {url}")
    req = Request(url, headers={"User-Agent": "lambda-ingest/1.0"})
    with urlopen(req, timeout=60) as r:
        data = r.read()
        if url.lower().endswith(".gz"):
            return gzip.decompress(data)
        return data

def _read_s3_bytes(uri: str) -> bytes:
    # s3://bucket/key
    p = urlparse(uri)
    if p.scheme != "s3":
        raise ValueError(f"Expected s3:// URI, got: {uri}")
    log.info(f"Fetching S3 source: {uri}")
    obj = s3.get_object(Bucket=p.netloc, Key=p.path.lstrip("/"))
    body = obj["Body"].read()
    if p.path.lower().endswith(".gz"):
        return gzip.decompress(body)
    return body

def _put_raw_to_bucket(raw_bytes: bytes, source_hint: str) -> str:
    """Store an exact raw copy for audit/replay."""
    today = _now_utc().date().isoformat()
    uid = uuid.uuid4().hex[:8]
    sha = hashlib.sha256(raw_bytes).hexdigest()[:12]
    key = f"raw/{today}/{int(time.time())}_{uid}_{sha}.dat"
    s3.put_object(
        Bucket=RAW_BUCKET,
        Key=key,
        Body=raw_bytes,
        ServerSideEncryption="AES256",
        Metadata={"source": source_hint},
        ContentType="application/octet-stream",
    )
    log.info(f"Stored raw dataset to s3://{RAW_BUCKET}/{key}")
    return key

def _normalize_date(s: str) -> str:
    """Return YYYY-MM-DD; supports 'YYYY-MM-DD', 'DD/MM/YYYY', 'MM/DD/YYYY', etc."""
    s = s.strip().replace(".", "-").replace("/", "-")
    try:
        return dt.date.fromisoformat(s).isoformat()
    except Exception:
        pass
    for fmt in ("%d-%m-%Y", "%m-%d-%Y", "%Y-%m-%d"):
        try:
            return dt.datetime.strptime(s, fmt).date().isoformat()
        except Exception:
            continue
    raise ValueError(f"Unrecognised date format: {s}")

def _parse_csv(raw: bytes):
    text = raw.decode("utf-8", errors="replace")
    rdr = csv.DictReader(io.StringIO(text))
    out = []
    for row in rdr:
        if not row:
            continue
        country = row.get("Country") or row.get("country") or row.get("location") or row.get("Country/Region")
        date = row.get("Date_reported") or row.get("date") or row.get("Date") or row.get("report_date")
        new_cases = row.get("New_cases") or row.get("new_cases") or row.get("cases") or row.get("new_cases_smoothed")
        new_deaths = row.get("New_deaths") or row.get("new_deaths") or row.get("deaths") or "0"
        if not (country and date):
            continue
        try:
            date_iso = _normalize_date(str(date))
        except Exception:
            continue
        def _to_int(v, default=0):
            try:
                return int(float(str(v).replace(",", "")))
            except Exception:
                return default
        out.append({
            "country": str(country).strip(),
            "date": date_iso,
            "new_cases": _to_int(new_cases, 0),
            "new_deaths": _to_int(new_deaths, 0),
        })
    return out

def _parse_json(raw: bytes):
    obj = json.loads(raw.decode("utf-8", errors="replace"))
    arr = obj["data"] if isinstance(obj, dict) and "data" in obj else obj
    out = []
    for it in arr:
        country = it.get("country") or it.get("Country") or it.get("location")
        date = it.get("date") or it.get("Date") or it.get("report_date")
        cases = it.get("new_cases") or it.get("New_cases") or it.get("cases")
        deaths = it.get("new_deaths") or it.get("New_deaths") or it.get("deaths")
        if not (country and date):
            continue
        try:
            date_iso = _normalize_date(str(date))
        except Exception:
            continue
        def _to_int(v, default=0):
            try:
                return int(float(str(v)))
            except Exception:
                return default
        out.append({
            "country": str(country).strip(),
            "date": date_iso,
            "new_cases": _to_int(cases, 0),
            "new_deaths": _to_int(deaths, 0),
        })
    return out

def _load_records(raw: bytes, hint_key: str):
    kind = _guess_content_type_from_key(hint_key)
    recs = _parse_json(raw) if kind == "json" else _parse_csv(raw)
    log.info(f"Parsed {len(recs)} records from {kind.upper()}")
    return recs

def _country_date_key(country: str, date_iso: str) -> str:
    return f"{country.strip().upper()}#{date_iso}"

def _write_ddb(records, mode: str = UPSERT_MODE):
    """
    Write to DynamoDB with explicit mode:
      - mode == "overwrite": fast batch_writer (overwrites existing items)
      - mode == "skip": idempotent inserts (skip if key exists)
    """
    written, skipped = 0, 0
    if mode == "overwrite":
        with ddb.batch_writer(overwrite_by_pkeys=("country_date",)) as bw:
            for r in records:
                bw.put_item(Item={
                    "country_date": _country_date_key(r["country"], r["date"]),
                    "country": r["country"],
                    "date": r["date"],
                    "new_cases": int(r.get("new_cases", 0)),
                    "new_deaths": int(r.get("new_deaths", 0)),
                    "ingested_at": _now_utc().isoformat()
                })
                written += 1
        return written, skipped

    # mode == "skip"
    for r in records:
        item = {
            "country_date": _country_date_key(r["country"], r["date"]),
            "country": r["country"],
            "date": r["date"],
            "new_cases": int(r.get("new_cases", 0)),
            "new_deaths": int(r.get("new_deaths", 0)),
            "ingested_at": _now_utc().isoformat()
        }
        try:
            ddb.put_item(Item=item, ConditionExpression="attribute_not_exists(country_date)")
            written += 1
        except ClientError as e:
            if e.response["Error"]["Code"] == "ConditionalCheckFailedException":
                skipped += 1
            else:
                raise
    return written, skipped

def _emit_metric(count: int, skipped: int, source: str):
    log.info(json.dumps({
        "metric": "IngestionSummary",
        "items_written": count,
        "items_skipped": skipped,
        "source": source,
        "table": DDB_TABLE,
        "bucket": RAW_BUCKET
    }))

def lambda_handler(event, context):
    """Event (all optional):
       {
         "source_url": "https://.../file.csv",
         "source_s3_uri": "s3://bucket/key",
         "upsert_mode": "skip" | "overwrite"
       }
    """
    log.info(f"Event: {json.dumps(event) if isinstance(event, dict) else str(event)}")

    src_url = (event or {}).get("source_url") or DEFAULT_SOURCE_URL
    src_s3 = (event or {}).get("source_s3_uri")
    mode = (event or {}).get("upsert_mode") or UPSERT_MODE

    # 1) Fetch
    if src_s3:
        raw = _read_s3_bytes(src_s3)
        source_hint, hint_key = src_s3, src_s3
    elif src_url:
        raw = _read_http_bytes(src_url)
        source_hint, hint_key = src_url, src_url
    else:
        raise ValueError("No source provided. Set SOURCE_URL or pass 'source_url'/'source_s3_uri' in the event.")

    # 2) Save raw copy
    raw_key = _put_raw_to_bucket(raw, source_hint)

    # 3) Parse
    records = _load_records(raw, hint_key=hint_key)
    if not records:
        log.warning("No records parsed; exiting.")
        return {"raw_key": raw_key, "written": 0, "skipped": 0}

    # 4) Write to DynamoDB (pass the mode explicitly)
    written, skipped = _write_ddb(records, mode=mode)

    # 5) Summary
    _emit_metric(written, skipped, source_hint)
    log.info(f"Done. Written={written}, Skipped={skipped}, Raw={raw_key}")
    return {"raw_key": raw_key, "written": written, "skipped": skipped}
